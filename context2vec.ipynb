{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "from keras.layers import TextVectorization\n",
    "from train import *\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "files = glob.glob('datag/Gutenberg/txt/*')\n",
    "text_ds = tf.data.TextLineDataset(files).filter(\n",
    "    lambda x: tf.cast(tf.strings.length(x), bool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62191\n"
     ]
    }
   ],
   "source": [
    "# vectorize the data\n",
    "sequence_length = 40\n",
    "vocab_size=5000\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=txt_eos_bos,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "vectorize_layer.adapt(text_ds.batch(1024))\n",
    "text_vecto_ds = text_ds.batch(1024).prefetch(\n",
    "    AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "sequences = list(text_vecto_ds.as_numpy_iterator())\n",
    "print(len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5000\n",
      "(183514, 4)\n",
      "(183514, 11)\n",
      "(183514, 11)\n"
     ]
    }
   ],
   "source": [
    "# create contexts, labels, targets\n",
    "# vocab_size = vectorize_layer.vocabulary_size()\n",
    "print(\"Vocab Size:\", vocab_size)\n",
    "window_size = 2\n",
    "num_ns = 10\n",
    "contexts, targets, labels = generate_train_data(sequences=sequences,\n",
    "                                                window_size=window_size,\n",
    "                                                num_ns=num_ns,\n",
    "                                                vocab_size=vocab_size,\n",
    "                                                seed=SEED)\n",
    "print(contexts.shape)\n",
    "print(targets.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((contexts, targets), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context2vec import Context2Vec\n",
    "model = Context2Vec(vocab_size=vocab_size,\n",
    "                    bilstm_hidden_units=50,\n",
    "                    ns=num_ns, input_units=100,\n",
    "                    embedding_dim=50,\n",
    "                    context_units=50, seq_length=sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, x_logit):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.losses.CategoricalCrossentropy(from_logits=True), \n",
    "#               metrics=['accuracy'])\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01),\n",
    "              loss=custom_loss, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "180/180 [==============================] - 8s 29ms/step - loss: 0.2863 - accuracy: 0.2754\n",
      "Epoch 2/20\n",
      "180/180 [==============================] - 5s 28ms/step - loss: 0.2460 - accuracy: 0.3667\n",
      "Epoch 3/20\n",
      "180/180 [==============================] - 5s 28ms/step - loss: 0.2241 - accuracy: 0.4449\n",
      "Epoch 4/20\n",
      "180/180 [==============================] - 5s 28ms/step - loss: 0.2108 - accuracy: 0.4922\n",
      "Epoch 5/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1982 - accuracy: 0.5389\n",
      "Epoch 6/20\n",
      "180/180 [==============================] - 5s 28ms/step - loss: 0.1863 - accuracy: 0.5824\n",
      "Epoch 7/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1757 - accuracy: 0.6173\n",
      "Epoch 8/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1673 - accuracy: 0.6479\n",
      "Epoch 9/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1598 - accuracy: 0.6721\n",
      "Epoch 10/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1528 - accuracy: 0.6952\n",
      "Epoch 11/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1472 - accuracy: 0.7130\n",
      "Epoch 12/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1421 - accuracy: 0.7299\n",
      "Epoch 13/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1374 - accuracy: 0.7447\n",
      "Epoch 14/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1335 - accuracy: 0.7571\n",
      "Epoch 15/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1293 - accuracy: 0.7685\n",
      "Epoch 16/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1256 - accuracy: 0.7798\n",
      "Epoch 17/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1227 - accuracy: 0.7874\n",
      "Epoch 18/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1196 - accuracy: 0.7967\n",
      "Epoch 19/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1166 - accuracy: 0.8049\n",
      "Epoch 20/20\n",
      "180/180 [==============================] - 5s 29ms/step - loss: 0.1138 - accuracy: 0.8121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21e7444f6d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('target_embeddings').get_weights()[0]\n",
    "words = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(words):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
